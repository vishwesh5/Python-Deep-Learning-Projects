{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training NN for Prediction using Regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishwesh5/Python-Deep-Learning-Projects/blob/master/Training_NN_for_Prediction_using_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "AvsQGbfArTko",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "9e533ec2-0712-495c-d87c-53cdb5631fd7"
      },
      "cell_type": "code",
      "source": [
        "# Download the data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data\",one_hot=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-33c6a74846b4>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F88-QmRzr6Hh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sample_image = mnist.train.images[0].reshape([28,28])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2qLZoinXrrO9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "b2165aae-419d-437a-d837-4598a8684466"
      },
      "cell_type": "code",
      "source": [
        "# Plot sample image\n",
        "import matplotlib.pyplot as plt\n",
        "plt.gray()\n",
        "plt.imshow(sample_image)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f04a4c77a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f04b8264f28>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "4vFvOtPVsGvL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "09012558-1930-47e1-bcd1-d87a2331f206"
      },
      "cell_type": "code",
      "source": [
        "mnist"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f04a598b3c8>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f04a5980ef0>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f04d203e908>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "0ab6jRRnsSOc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "571ea5fa-bf6f-43aa-e933-d475967efa1a"
      },
      "cell_type": "code",
      "source": [
        "mnist.validation.labels.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "HrMPnR5xsYGu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79fdb4dc-c9f7-4c90-c2cc-3d162e77484e"
      },
      "cell_type": "code",
      "source": [
        "mnist.validation.images.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "N0PE_IBvsZ3Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "learning_rate = 0.01\n",
        "num_steps = 100\n",
        "batch_size = 128\n",
        "display_step = 1\n",
        "# Network Parameters\n",
        "n_hidden_1 = 300 # 1st layer number of neurons\n",
        "n_hidden_2 = 300 # 2nd layer number of neurons\n",
        "num_input = 784 # MNIST data input (img shape: 28*28)\n",
        "num_classes = 10 # MNIST total classes (0-9 digits)\n",
        "#Training Parameters\n",
        "checkpoint_every = 100\n",
        "checkpoint_dir = './runs/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gAMJ2Ig9tCxt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B2q3KiMztKB_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(\"float\", [None, num_input],name=\"input_x\")\n",
        "Y = tf.placeholder(\"float\", [None, num_classes],name=\"input_y\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nTvXQzvDtPzq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weights = {'w1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\\\n",
        "           'w2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\\\n",
        "           'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
        "          }\n",
        "biases = {'b1': tf.Variable(tf.random_normal([n_hidden_1])),\\\n",
        "           'b2': tf.Variable(tf.random_normal([n_hidden_2])),\\\n",
        "           'out': tf.Variable(tf.random_normal([num_classes]))\n",
        "          }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iw823Coftis6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "layer_1 = tf.add(tf.matmul(X, weights['w1']), biases['b1'])\n",
        "layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "logits = tf.matmul(layer_2, weights['out']) + biases['out']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AH8BMR4Ytz3s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prediction = tf.nn.softmax(logits, name='prediction')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z6ekRQTPt2vn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "7a00c00d-5acb-49ba-98a5-885d2797daf7"
      },
      "cell_type": "code",
      "source": [
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-16-cbe74f179ab4>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T419rxM1uEtq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32) ,name='accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HLTpOvV5uQyf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "81WnqXttulMf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_dir = os.path.abspath(os.path.join(checkpoint_dir, \"checkpoints\"))\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "  os.makedirs(checkpoint_dir)\n",
        "\n",
        "# We only keep the last 2 checkpoints to manage storage\n",
        "saver = tf.train.Saver(tf.global_variables(), max_to_keep=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nZ9citoWun0w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize the variables\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_KqJybCjurha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1751
        },
        "outputId": "c626f6ba-f543-4f89-c453-30a8336e1f15"
      },
      "cell_type": "code",
      "source": [
        "all_loss = []\n",
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "    writer_1 = tf.summary.FileWriter(\"./runs/summary/\",sess.graph)\n",
        "\n",
        "    sum_var = tf.summary.scalar(\"loss\", accuracy)\n",
        "    write_op = tf.summary.merge_all()\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    for step in range(1, num_steps+1):\n",
        "        # Extracting \n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc, summary = sess.run([loss_op, accuracy, write_op], feed_dict={X: batch_x,\\\n",
        "                                                                 Y: batch_y})\n",
        "            all_loss.append(loss)\n",
        "            writer_1.add_summary(summary, step)\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc))\n",
        "        if step % checkpoint_every == 0:\n",
        "            path = saver.save(\\\n",
        "                        sess, checkpoint_prefix, global_step=step)\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Calculate accuracy for MNIST test images\n",
        "    print(\"Testing Accuracy:\", \\\n",
        "        sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
        "                                      Y: mnist.test.labels}))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1, Minibatch Loss= 2628.6326, Training Accuracy= 0.211\n",
            "Step 2, Minibatch Loss= 1889.8379, Training Accuracy= 0.328\n",
            "Step 3, Minibatch Loss= 1940.7211, Training Accuracy= 0.352\n",
            "Step 4, Minibatch Loss= 1464.7080, Training Accuracy= 0.453\n",
            "Step 5, Minibatch Loss= 1453.5039, Training Accuracy= 0.461\n",
            "Step 6, Minibatch Loss= 950.1848, Training Accuracy= 0.664\n",
            "Step 7, Minibatch Loss= 964.3688, Training Accuracy= 0.602\n",
            "Step 8, Minibatch Loss= 763.7780, Training Accuracy= 0.625\n",
            "Step 9, Minibatch Loss= 671.0916, Training Accuracy= 0.680\n",
            "Step 10, Minibatch Loss= 514.6049, Training Accuracy= 0.719\n",
            "Step 11, Minibatch Loss= 540.8394, Training Accuracy= 0.727\n",
            "Step 12, Minibatch Loss= 623.8715, Training Accuracy= 0.758\n",
            "Step 13, Minibatch Loss= 536.1475, Training Accuracy= 0.727\n",
            "Step 14, Minibatch Loss= 744.5597, Training Accuracy= 0.703\n",
            "Step 15, Minibatch Loss= 758.3807, Training Accuracy= 0.703\n",
            "Step 16, Minibatch Loss= 715.1078, Training Accuracy= 0.781\n",
            "Step 17, Minibatch Loss= 429.5332, Training Accuracy= 0.812\n",
            "Step 18, Minibatch Loss= 459.4146, Training Accuracy= 0.758\n",
            "Step 19, Minibatch Loss= 344.2949, Training Accuracy= 0.766\n",
            "Step 20, Minibatch Loss= 267.7400, Training Accuracy= 0.828\n",
            "Step 21, Minibatch Loss= 287.7295, Training Accuracy= 0.805\n",
            "Step 22, Minibatch Loss= 349.8679, Training Accuracy= 0.797\n",
            "Step 23, Minibatch Loss= 458.6699, Training Accuracy= 0.828\n",
            "Step 24, Minibatch Loss= 444.9017, Training Accuracy= 0.812\n",
            "Step 25, Minibatch Loss= 653.2936, Training Accuracy= 0.750\n",
            "Step 26, Minibatch Loss= 222.2930, Training Accuracy= 0.867\n",
            "Step 27, Minibatch Loss= 438.3331, Training Accuracy= 0.828\n",
            "Step 28, Minibatch Loss= 380.9294, Training Accuracy= 0.812\n",
            "Step 29, Minibatch Loss= 393.9484, Training Accuracy= 0.812\n",
            "Step 30, Minibatch Loss= 204.6024, Training Accuracy= 0.875\n",
            "Step 31, Minibatch Loss= 176.0053, Training Accuracy= 0.930\n",
            "Step 32, Minibatch Loss= 146.9448, Training Accuracy= 0.875\n",
            "Step 33, Minibatch Loss= 259.6106, Training Accuracy= 0.867\n",
            "Step 34, Minibatch Loss= 243.9684, Training Accuracy= 0.859\n",
            "Step 35, Minibatch Loss= 324.9766, Training Accuracy= 0.852\n",
            "Step 36, Minibatch Loss= 323.1364, Training Accuracy= 0.891\n",
            "Step 37, Minibatch Loss= 406.1660, Training Accuracy= 0.883\n",
            "Step 38, Minibatch Loss= 341.0065, Training Accuracy= 0.836\n",
            "Step 39, Minibatch Loss= 283.7361, Training Accuracy= 0.852\n",
            "Step 40, Minibatch Loss= 246.1722, Training Accuracy= 0.883\n",
            "Step 41, Minibatch Loss= 185.4200, Training Accuracy= 0.867\n",
            "Step 42, Minibatch Loss= 348.1456, Training Accuracy= 0.836\n",
            "Step 43, Minibatch Loss= 344.1907, Training Accuracy= 0.805\n",
            "Step 44, Minibatch Loss= 365.9361, Training Accuracy= 0.812\n",
            "Step 45, Minibatch Loss= 252.3572, Training Accuracy= 0.859\n",
            "Step 46, Minibatch Loss= 310.6758, Training Accuracy= 0.820\n",
            "Step 47, Minibatch Loss= 191.5130, Training Accuracy= 0.844\n",
            "Step 48, Minibatch Loss= 319.1008, Training Accuracy= 0.797\n",
            "Step 49, Minibatch Loss= 270.8232, Training Accuracy= 0.828\n",
            "Step 50, Minibatch Loss= 140.9810, Training Accuracy= 0.883\n",
            "Step 51, Minibatch Loss= 356.4887, Training Accuracy= 0.820\n",
            "Step 52, Minibatch Loss= 331.0933, Training Accuracy= 0.867\n",
            "Step 53, Minibatch Loss= 162.4936, Training Accuracy= 0.922\n",
            "Step 54, Minibatch Loss= 238.8484, Training Accuracy= 0.891\n",
            "Step 55, Minibatch Loss= 314.6006, Training Accuracy= 0.812\n",
            "Step 56, Minibatch Loss= 188.7762, Training Accuracy= 0.906\n",
            "Step 57, Minibatch Loss= 378.9347, Training Accuracy= 0.844\n",
            "Step 58, Minibatch Loss= 226.0513, Training Accuracy= 0.883\n",
            "Step 59, Minibatch Loss= 298.1822, Training Accuracy= 0.852\n",
            "Step 60, Minibatch Loss= 231.0655, Training Accuracy= 0.844\n",
            "Step 61, Minibatch Loss= 172.6861, Training Accuracy= 0.875\n",
            "Step 62, Minibatch Loss= 137.9470, Training Accuracy= 0.852\n",
            "Step 63, Minibatch Loss= 252.5726, Training Accuracy= 0.859\n",
            "Step 64, Minibatch Loss= 171.9527, Training Accuracy= 0.852\n",
            "Step 65, Minibatch Loss= 400.7441, Training Accuracy= 0.852\n",
            "Step 66, Minibatch Loss= 197.0551, Training Accuracy= 0.828\n",
            "Step 67, Minibatch Loss= 268.6307, Training Accuracy= 0.828\n",
            "Step 68, Minibatch Loss= 320.0492, Training Accuracy= 0.820\n",
            "Step 69, Minibatch Loss= 151.2557, Training Accuracy= 0.859\n",
            "Step 70, Minibatch Loss= 171.7382, Training Accuracy= 0.891\n",
            "Step 71, Minibatch Loss= 223.4520, Training Accuracy= 0.867\n",
            "Step 72, Minibatch Loss= 136.2067, Training Accuracy= 0.875\n",
            "Step 73, Minibatch Loss= 319.7961, Training Accuracy= 0.836\n",
            "Step 74, Minibatch Loss= 222.4612, Training Accuracy= 0.891\n",
            "Step 75, Minibatch Loss= 138.6024, Training Accuracy= 0.891\n",
            "Step 76, Minibatch Loss= 405.4775, Training Accuracy= 0.812\n",
            "Step 77, Minibatch Loss= 67.4031, Training Accuracy= 0.906\n",
            "Step 78, Minibatch Loss= 62.4500, Training Accuracy= 0.914\n",
            "Step 79, Minibatch Loss= 288.4822, Training Accuracy= 0.859\n",
            "Step 80, Minibatch Loss= 343.1119, Training Accuracy= 0.812\n",
            "Step 81, Minibatch Loss= 422.9701, Training Accuracy= 0.781\n",
            "Step 82, Minibatch Loss= 147.5688, Training Accuracy= 0.859\n",
            "Step 83, Minibatch Loss= 175.2802, Training Accuracy= 0.891\n",
            "Step 84, Minibatch Loss= 114.2350, Training Accuracy= 0.906\n",
            "Step 85, Minibatch Loss= 144.6583, Training Accuracy= 0.875\n",
            "Step 86, Minibatch Loss= 137.7175, Training Accuracy= 0.914\n",
            "Step 87, Minibatch Loss= 188.0366, Training Accuracy= 0.859\n",
            "Step 88, Minibatch Loss= 152.5060, Training Accuracy= 0.875\n",
            "Step 89, Minibatch Loss= 179.8382, Training Accuracy= 0.859\n",
            "Step 90, Minibatch Loss= 327.1851, Training Accuracy= 0.867\n",
            "Step 91, Minibatch Loss= 202.5509, Training Accuracy= 0.836\n",
            "Step 92, Minibatch Loss= 137.5613, Training Accuracy= 0.914\n",
            "Step 93, Minibatch Loss= 73.7978, Training Accuracy= 0.906\n",
            "Step 94, Minibatch Loss= 196.4191, Training Accuracy= 0.883\n",
            "Step 95, Minibatch Loss= 251.7174, Training Accuracy= 0.852\n",
            "Step 96, Minibatch Loss= 190.3773, Training Accuracy= 0.852\n",
            "Step 97, Minibatch Loss= 136.2379, Training Accuracy= 0.891\n",
            "Step 98, Minibatch Loss= 165.8359, Training Accuracy= 0.875\n",
            "Step 99, Minibatch Loss= 226.5544, Training Accuracy= 0.867\n",
            "Step 100, Minibatch Loss= 309.6909, Training Accuracy= 0.828\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.8828\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i3D5O4J0u94q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}